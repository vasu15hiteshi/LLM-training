{
	"cells": [
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Load the tokenizer"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 2,
			"metadata": {},
			"outputs": [],
			"source": [
				"import sys\n",
				"sys.path.append('..')"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 1,
			"metadata": {},
			"outputs": [],
			"source": [
				"from minbpe import BasicTokenizer\n",
				"\n",
				"tokenizer = BasicTokenizer()\n",
				"tokenizer.load(model_file=\"/Users/vasuhiteshi/Documents/llm/tokenizer/my_tokenizer.model\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 3,
			"metadata": {},
			"outputs": [],
			"source": [
				"def get_vocab_size(tokenizer: BasicTokenizer) -> int:\n",
				"    vocab = tokenizer.vocab\n",
				"    special_tokens = tokenizer.special_tokens\n",
				"\n",
				"    return len(vocab) + len(special_tokens)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Creating the model"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"This model is based on [Andrej Karpathy's](https://github.com/karpathy/ng-video-lecture/blob/master/gpt.py) implementation of the GPT-2 model."
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Step 1 : Word & position embedding"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"In this step, we convert the text into a list of tokens. Each token has an ID from the vocabulary. The shape of the tensor is 1x6 because we have one sentence with 6 tokens.\n",
				"\n",
				"Next, we use these tokens to find the corresponding embedding vector for each token. The vocabulary size is 1024, so each token uses its ID to look up the right vector in the token embedding table. We do the same for positional embeddings, which have 256 rows because the block size is 256. This means the model can only handle sequences with up to 256 tokens.\n",
				"\n",
				"After getting the token and positional embeddings, we add them together. This results in a tensor of size 1x6x768, where 1 is the number of inputs, 6 is the number of tokens, and 768 is the size of the embedding vectors. This output is then sent to the block layer."
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"![step_1](../images/transformer_step_1.svg)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Step 2: Multi-Head attention"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"We take the tensor from the previous step and pass it to the multi-head attention layer. This layer has two settings: head size and number of heads. These settings split the attention block into smaller parts called heads. All heads process the input at the same time to speed up calculations.  \n",
				"\n",
				"The goal of multi-head attention is to help the model focus on different parts of the input at once. Each head can learn to look at different relationships between words or tokens. Since they work in parallel, the model can understand patterns in the data more effectively.  \n",
				"\n",
				"Each head produces a tensor of size **1x6x128**, where **6** is the number of heads and **128** is the size of each head. We then combine all the outputs into a **1x6x768** tensor. Finally, this is passed through a feed-forward layer, which adjusts the last dimension to **768**, matching the embedding size.\n",
				"\n",
				"We can stack multiple multi-head attention blocks to deepen the model's understanding of the input. This allows it to learn more complex patterns and relationships. In the image, we have stacked four layers to enhance its ability to process the data."
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"![step_2](../images/transformer_step_2.svg)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Step 3: Prediction layer"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"After the final attention block, the output is passed to the prediction layer. This layer contains a dense layer that reshapes the output to **1x6x1024**, matching the vocabulary size. In the end, we get a probability distribution, allowing us to sample the next token in the sequence."
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"![step_3](../images/transformer_step_3.svg)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Code"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 5,
			"metadata": {},
			"outputs": [],
			"source": [
				"import torch\n",
				"torch.manual_seed(3647)\n",
				"\n",
				"block_size = 256\n",
				"n_embd = 384\n",
				"n_head = 6\n",
				"n_layer = 6\n",
				"dropout = 0.2\n",
				"vocab_size = get_vocab_size(tokenizer)\n",
				"device = 'cuda' if torch.cuda.is_available() else 'cpu'"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### 1. Head"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 6,
			"metadata": {},
			"outputs": [],
			"source": [
				"from typing import Optional, Tuple\n",
				"import torch\n",
				"import torch.nn as nn\n",
				"from torch.nn import functional as F\n",
				"\n",
				"\n",
				"class Head(nn.Module):\n",
				"    \"\"\" one head of self-attention \"\"\"\n",
				"\n",
				"    def __init__(self, head_size: int) -> None:\n",
				"        super().__init__()\n",
				"        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
				"        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
				"        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
				"        self.register_buffer('tril', torch.tril(\n",
				"            torch.ones(block_size, block_size)))\n",
				"\n",
				"        self.dropout = nn.Dropout(dropout)\n",
				"\n",
				"    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
				"        # input of size (batch, time-step, channels)\n",
				"        # output of size (batch, time-step, head size)\n",
				"        _, T, _ = x.shape\n",
				"        k = self.key(x)   # (B,T,hs)\n",
				"        q = self.query(x)  # (B,T,hs)\n",
				"        # compute attention scores (\"affinities\")\n",
				"        # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
				"        weights = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n",
				"        weights = weights.masked_fill(\n",
				"            self.tril[:T, :T] == 0, float('-inf'))  # (B, T, T)\n",
				"        weights = F.softmax(weights, dim=-1)  # (B, T, T)\n",
				"        weights = self.dropout(weights)\n",
				"        # perform the weighted aggregation of the values\n",
				"        v = self.value(x)  # (B,T,hs)\n",
				"        out = weights @ v  # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
				"        return out"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### 2. Multi Head Attention"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 7,
			"metadata": {},
			"outputs": [],
			"source": [
				"class MultiHeadAttention(nn.Module):\n",
				"    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
				"\n",
				"    def __init__(self, num_heads: int, head_size: int) -> None:\n",
				"        super().__init__()\n",
				"        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
				"        self.projection = nn.Linear(head_size * num_heads, n_embd)\n",
				"        self.dropout = nn.Dropout(dropout)\n",
				"\n",
				"    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
				"        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
				"        out = self.dropout(self.projection(out))\n",
				"        return out"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### 3. Block"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 8,
			"metadata": {},
			"outputs": [],
			"source": [
				"class FeedForward(nn.Module):\n",
				"    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
				"\n",
				"    def __init__(self, n_embd: int) -> None:\n",
				"        super().__init__()\n",
				"        self.net = nn.Sequential(\n",
				"            nn.Linear(n_embd, 4 * n_embd),\n",
				"            nn.ReLU(),\n",
				"            nn.Linear(4 * n_embd, n_embd),\n",
				"            nn.Dropout(dropout),\n",
				"        )\n",
				"\n",
				"    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
				"        return self.net(x)\n",
				"\n",
				"\n",
				"class Block(nn.Module):\n",
				"    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
				"\n",
				"    def __init__(self, n_embd: int, n_head: int) -> None:\n",
				"        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
				"        super().__init__()\n",
				"        head_size = n_embd // n_head\n",
				"        self.self_attention = MultiHeadAttention(n_head, head_size)\n",
				"        self.feed_forward = FeedForward(n_embd)\n",
				"        self.layer_norm_1 = nn.LayerNorm(n_embd)\n",
				"        self.layer_norm_2 = nn.LayerNorm(n_embd)\n",
				"\n",
				"    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
				"        x = x + self.self_attention(self.layer_norm_1(x))\n",
				"        x = x + self.feed_forward(self.layer_norm_2(x))\n",
				"        return x"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### 4. Assembling the language model"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 9,
			"metadata": {},
			"outputs": [],
			"source": [
				"class GPTLanguageModel(nn.Module):\n",
				"\n",
				"    def __init__(self) -> None:\n",
				"        super().__init__()\n",
				"        # each token directly reads off the logits for the next token from a lookup table\n",
				"        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
				"        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
				"        self.blocks = nn.Sequential(\n",
				"            *[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
				"        self.final_layer_norm = nn.LayerNorm(n_embd)\n",
				"        self.final_linear_layer = nn.Linear(n_embd, vocab_size)\n",
				"\n",
				"        self.apply(self._init_weights)\n",
				"\n",
				"    def _init_weights(self, module: nn.Module) -> None:\n",
				"        if isinstance(module, nn.Linear):\n",
				"            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
				"            if module.bias is not None:\n",
				"                torch.nn.init.zeros_(module.bias)\n",
				"        elif isinstance(module, nn.Embedding):\n",
				"            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
				"\n",
				"    def forward(self, input_tokens: torch.Tensor, targets: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
				"        \"\"\"\n",
				"        Forward pass of the model.\n",
				"\n",
				"        Args:\n",
				"            input_tokens: Tensor of token indices of shape (batch_size, sequence_length)\n",
				"            targets: Optional tensor of target token indices of same shape as input_tokens\n",
				"\n",
				"        Returns:\n",
				"            Tuple of (logits, loss) where logits has shape (batch_size, sequence_length, vocab_size)\n",
				"            and loss is optional cross-entropy loss if targets are provided\n",
				"        \"\"\"\n",
				"\n",
				"        B, T = input_tokens.shape\n",
				"\n",
				"        # input_tokens and targets are both (B,T) tensor of integers\n",
				"        token_embedding = self.token_embedding_table(input_tokens)  # (B,T,C)\n",
				"        positional_embedding = self.position_embedding_table(\n",
				"            torch.arange(T, device=device))  # (T,C)\n",
				"        x = token_embedding + positional_embedding  # (B,T,C)\n",
				"        x = self.blocks(x)  # (B,T,C)\n",
				"        x = self.final_layer_norm(x)  # (B,T,C)\n",
				"        logits = self.final_linear_layer(x)  # (B,T,vocab_size)\n",
				"\n",
				"        if targets is None:\n",
				"            loss = None\n",
				"        else:\n",
				"            B, T, C = logits.shape\n",
				"            logits = logits.view(B*T, C)\n",
				"            targets = targets.view(B*T)\n",
				"            loss = F.cross_entropy(logits, targets)\n",
				"\n",
				"        return logits, loss\n",
				"\n",
				"    def generate(self, input_tokens: torch.Tensor, max_new_tokens: int) -> torch.Tensor:\n",
				"        \"\"\"\n",
				"                Generate new tokens given a context.\n",
				"\n",
				"                Args:>ns: Starting token indices of shape (batch_size, sequence_length)\n",
				"                        max_new_tokens: Number of new tokens to generate\n",
				"\n",
				"                Returns:\n",
				"                        Tensor of token indices of shape (batch_size, sequence_length + max_new_tokens)\n",
				"                \"\"\"\n",
				"\n",
				"        # input_tokens is (B, T) array of indices in the current context\n",
				"        for _ in range(max_new_tokens):\n",
				"            # crop input_tokens to the last block_size tokens\n",
				"            cropped_input = input_tokens[:, -block_size:]\n",
				"            # get the predictions\n",
				"            logits, _ = self(cropped_input)\n",
				"            # focus only on the last time step\n",
				"            logits = logits[:, -1, :]  # becomes (B, C)\n",
				"            # apply softmax to get probabilities\n",
				"            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
				"            # sample from the distribution\n",
				"            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
				"            # append sampled index to the running sequence\n",
				"            input_tokens = torch.cat(\n",
				"                (input_tokens, idx_next), dim=1)  # (B, T+1)\n",
				"        return input_tokens"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### 5. Parameters & dummy input"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 10,
			"metadata": {},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"11.53409 M parameters\n"
					]
				}
			],
			"source": [
				"model = GPTLanguageModel()\n",
				"model = model.to(device)\n",
				"# print the number of parameters in the model\n",
				"print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 11,
			"metadata": {},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"torch.Size([1, 6, 1034]) None\n"
					]
				}
			],
			"source": [
				"batch_size = 1\n",
				"seq_length = 6\n",
				"x = torch.randint(0, vocab_size, (batch_size, seq_length))\n",
				"x = x.to(device)\n",
				"\n",
				"logits, loss = model(x)\n",
				"print(logits.shape, loss)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Display the model summary"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 12,
			"metadata": {},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"├─ token_embedding_table: Embedding (397,056 parameters)\n",
						"├─ position_embedding_table: Embedding (98,304 parameters)\n",
						"├─ blocks: Sequential (10,639,872 parameters)\n",
						"│  ├─ 0: Block (1,773,312 parameters)\n",
						"│  │  ├─ self_attention: MultiHeadAttention (590,208 parameters)\n",
						"│  │  │  ├─ heads: ModuleList (442,368 parameters)\n",
						"│  │  │  │  ├─ 0: Head (73,728 parameters)\n",
						"│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
						"│  │  │  │  ├─ 1: Head (73,728 parameters)\n",
						"│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
						"│  │  │  │  ├─ 2: Head (73,728 parameters)\n",
						"│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
						"│  │  │  │  ├─ 3: Head (73,728 parameters)\n",
						"│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
						"│  │  │  │  ├─ 4: Head (73,728 parameters)\n",
						"│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
						"│  │  │  │  ├─ 5: Head (73,728 parameters)\n",
						"│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
						"│  │  │  ├─ projection: Linear (147,840 parameters)\n",
						"│  │  │  ├─ dropout: Dropout (0 parameters)\n",
						"│  │  ├─ feed_forward: FeedForward (1,181,568 parameters)\n",
						"│  │  │  ├─ net: Sequential (1,181,568 parameters)\n",
						"│  │  │  │  ├─ 0: Linear (591,360 parameters)\n",
						"│  │  │  │  ├─ 1: ReLU (0 parameters)\n",
						"│  │  │  │  ├─ 2: Linear (590,208 parameters)\n",
						"│  │  │  │  ├─ 3: Dropout (0 parameters)\n",
						"│  │  ├─ layer_norm_1: LayerNorm (768 parameters)\n",
						"│  │  ├─ layer_norm_2: LayerNorm (768 parameters)\n",
						"│  ├─ 1: Block (1,773,312 parameters)\n",
						"│  │  ├─ self_attention: MultiHeadAttention (590,208 parameters)\n",
						"│  │  │  ├─ heads: ModuleList (442,368 parameters)\n",
						"│  │  │  │  ├─ 0: Head (73,728 parameters)\n",
						"│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
						"│  │  │  │  ├─ 1: Head (73,728 parameters)\n",
						"│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
						"│  │  │  │  ├─ 2: Head (73,728 parameters)\n",
						"│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
						"│  │  │  │  ├─ 3: Head (73,728 parameters)\n",
						"│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
						"│  │  │  │  ├─ 4: Head (73,728 parameters)\n",
						"│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
						"│  │  │  │  ├─ 5: Head (73,728 parameters)\n",
						"│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
						"│  │  │  ├─ projection: Linear (147,840 parameters)\n",
						"│  │  │  ├─ dropout: Dropout (0 parameters)\n",
						"│  │  ├─ feed_forward: FeedForward (1,181,568 parameters)\n",
						"│  │  │  ├─ net: Sequential (1,181,568 parameters)\n",
						"│  │  │  │  ├─ 0: Linear (591,360 parameters)\n",
						"│  │  │  │  ├─ 1: ReLU (0 parameters)\n",
						"│  │  │  │  ├─ 2: Linear (590,208 parameters)\n",
						"│  │  │  │  ├─ 3: Dropout (0 parameters)\n",
						"│  │  ├─ layer_norm_1: LayerNorm (768 parameters)\n",
						"│  │  ├─ layer_norm_2: LayerNorm (768 parameters)\n",
						"│  ├─ 2: Block (1,773,312 parameters)\n",
						"│  │  ├─ self_attention: MultiHeadAttention (590,208 parameters)\n",
						"│  │  │  ├─ heads: ModuleList (442,368 parameters)\n",
						"│  │  │  │  ├─ 0: Head (73,728 parameters)\n",
						"│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
						"│  │  │  │  ├─ 1: Head (73,728 parameters)\n",
						"│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
						"│  │  │  │  ├─ 2: Head (73,728 parameters)\n",
						"│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
						"│  │  │  │  ├─ 3: Head (73,728 parameters)\n",
						"│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
						"│  │  │  │  ├─ 4: Head (73,728 parameters)\n",
						"│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
						"│  │  │  │  ├─ 5: Head (73,728 parameters)\n",
						"│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
						"│  │  │  ├─ projection: Linear (147,840 parameters)\n",
						"│  │  │  ├─ dropout: Dropout (0 parameters)\n",
						"│  │  ├─ feed_forward: FeedForward (1,181,568 parameters)\n",
						"│  │  │  ├─ net: Sequential (1,181,568 parameters)\n",
						"│  │  │  │  ├─ 0: Linear (591,360 parameters)\n",
						"│  │  │  │  ├─ 1: ReLU (0 parameters)\n",
						"│  │  │  │  ├─ 2: Linear (590,208 parameters)\n",
						"│  │  │  │  ├─ 3: Dropout (0 parameters)\n",
						"│  │  ├─ layer_norm_1: LayerNorm (768 parameters)\n",
						"│  │  ├─ layer_norm_2: LayerNorm (768 parameters)\n",
						"│  ├─ 3: Block (1,773,312 parameters)\n",
						"│  │  ├─ self_attention: MultiHeadAttention (590,208 parameters)\n",
						"│  │  │  ├─ heads: ModuleList (442,368 parameters)\n",
						"│  │  │  │  ├─ 0: Head (73,728 parameters)\n",
						"│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
						"│  │  │  │  ├─ 1: Head (73,728 parameters)\n",
						"│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
						"│  │  │  │  ├─ 2: Head (73,728 parameters)\n",
						"│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
						"│  │  │  │  ├─ 3: Head (73,728 parameters)\n",
						"│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
						"│  │  │  │  ├─ 4: Head (73,728 parameters)\n",
						"│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
						"│  │  │  │  ├─ 5: Head (73,728 parameters)\n",
						"│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
						"│  │  │  ├─ projection: Linear (147,840 parameters)\n",
						"│  │  │  ├─ dropout: Dropout (0 parameters)\n",
						"│  │  ├─ feed_forward: FeedForward (1,181,568 parameters)\n",
						"│  │  │  ├─ net: Sequential (1,181,568 parameters)\n",
						"│  │  │  │  ├─ 0: Linear (591,360 parameters)\n",
						"│  │  │  │  ├─ 1: ReLU (0 parameters)\n",
						"│  │  │  │  ├─ 2: Linear (590,208 parameters)\n",
						"│  │  │  │  ├─ 3: Dropout (0 parameters)\n",
						"│  │  ├─ layer_norm_1: LayerNorm (768 parameters)\n",
						"│  │  ├─ layer_norm_2: LayerNorm (768 parameters)\n",
						"│  ├─ 4: Block (1,773,312 parameters)\n",
						"│  │  ├─ self_attention: MultiHeadAttention (590,208 parameters)\n",
						"│  │  │  ├─ heads: ModuleList (442,368 parameters)\n",
						"│  │  │  │  ├─ 0: Head (73,728 parameters)\n",
						"│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
						"│  │  │  │  ├─ 1: Head (73,728 parameters)\n",
						"│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
						"│  │  │  │  ├─ 2: Head (73,728 parameters)\n",
						"│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
						"│  │  │  │  ├─ 3: Head (73,728 parameters)\n",
						"│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
						"│  │  │  │  ├─ 4: Head (73,728 parameters)\n",
						"│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
						"│  │  │  │  ├─ 5: Head (73,728 parameters)\n",
						"│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
						"│  │  │  ├─ projection: Linear (147,840 parameters)\n",
						"│  │  │  ├─ dropout: Dropout (0 parameters)\n",
						"│  │  ├─ feed_forward: FeedForward (1,181,568 parameters)\n",
						"│  │  │  ├─ net: Sequential (1,181,568 parameters)\n",
						"│  │  │  │  ├─ 0: Linear (591,360 parameters)\n",
						"│  │  │  │  ├─ 1: ReLU (0 parameters)\n",
						"│  │  │  │  ├─ 2: Linear (590,208 parameters)\n",
						"│  │  │  │  ├─ 3: Dropout (0 parameters)\n",
						"│  │  ├─ layer_norm_1: LayerNorm (768 parameters)\n",
						"│  │  ├─ layer_norm_2: LayerNorm (768 parameters)\n",
						"│  ├─ 5: Block (1,773,312 parameters)\n",
						"│  │  ├─ self_attention: MultiHeadAttention (590,208 parameters)\n",
						"│  │  │  ├─ heads: ModuleList (442,368 parameters)\n",
						"│  │  │  │  ├─ 0: Head (73,728 parameters)\n",
						"│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
						"│  │  │  │  ├─ 1: Head (73,728 parameters)\n",
						"│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
						"│  │  │  │  ├─ 2: Head (73,728 parameters)\n",
						"│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
						"│  │  │  │  ├─ 3: Head (73,728 parameters)\n",
						"│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
						"│  │  │  │  ├─ 4: Head (73,728 parameters)\n",
						"│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
						"│  │  │  │  ├─ 5: Head (73,728 parameters)\n",
						"│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
						"│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
						"│  │  │  ├─ projection: Linear (147,840 parameters)\n",
						"│  │  │  ├─ dropout: Dropout (0 parameters)\n",
						"│  │  ├─ feed_forward: FeedForward (1,181,568 parameters)\n",
						"│  │  │  ├─ net: Sequential (1,181,568 parameters)\n",
						"│  │  │  │  ├─ 0: Linear (591,360 parameters)\n",
						"│  │  │  │  ├─ 1: ReLU (0 parameters)\n",
						"│  │  │  │  ├─ 2: Linear (590,208 parameters)\n",
						"│  │  │  │  ├─ 3: Dropout (0 parameters)\n",
						"│  │  ├─ layer_norm_1: LayerNorm (768 parameters)\n",
						"│  │  ├─ layer_norm_2: LayerNorm (768 parameters)\n",
						"├─ final_layer_norm: LayerNorm (768 parameters)\n",
						"├─ final_linear_layer: Linear (398,090 parameters)\n"
					]
				}
			],
			"source": [
				"def print_model_structure(model: torch.nn.Module, indent: str = '') -> None:\n",
				"    \"\"\"\n",
				"    Custom function to print model structure in a hierarchical format\n",
				"    \"\"\"\n",
				"    for name, child in model.named_children():\n",
				"        params = sum(p.numel() for p in child.parameters())\n",
				"        print(f\"{indent}├─ {name}: {child.__class__.__name__} ({params:,} parameters)\")\n",
				"        print_model_structure(child, indent + '│  ')\n",
				"\n",
				"\n",
				"print_model_structure(model)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 13,
			"metadata": {},
			"outputs": [
				{
					"data": {
						"text/html": [
							"<div>\n",
							"<style scoped>\n",
							"    .dataframe tbody tr th:only-of-type {\n",
							"        vertical-align: middle;\n",
							"    }\n",
							"\n",
							"    .dataframe tbody tr th {\n",
							"        vertical-align: top;\n",
							"    }\n",
							"\n",
							"    .dataframe thead th {\n",
							"        text-align: right;\n",
							"    }\n",
							"</style>\n",
							"<table border=\"1\" class=\"dataframe\">\n",
							"  <thead>\n",
							"    <tr style=\"text-align: right;\">\n",
							"      <th></th>\n",
							"      <th>Layer Name</th>\n",
							"      <th>Type</th>\n",
							"      <th>Parameters</th>\n",
							"      <th>Trainable</th>\n",
							"    </tr>\n",
							"  </thead>\n",
							"  <tbody>\n",
							"    <tr>\n",
							"      <th>0</th>\n",
							"      <td>token_embedding_table</td>\n",
							"      <td>Embedding</td>\n",
							"      <td>397056</td>\n",
							"      <td>397056</td>\n",
							"    </tr>\n",
							"    <tr>\n",
							"      <th>1</th>\n",
							"      <td>position_embedding_table</td>\n",
							"      <td>Embedding</td>\n",
							"      <td>98304</td>\n",
							"      <td>98304</td>\n",
							"    </tr>\n",
							"    <tr>\n",
							"      <th>2</th>\n",
							"      <td>blocks.0.self_attention.heads.0.key</td>\n",
							"      <td>Linear</td>\n",
							"      <td>24576</td>\n",
							"      <td>24576</td>\n",
							"    </tr>\n",
							"    <tr>\n",
							"      <th>3</th>\n",
							"      <td>blocks.0.self_attention.heads.0.query</td>\n",
							"      <td>Linear</td>\n",
							"      <td>24576</td>\n",
							"      <td>24576</td>\n",
							"    </tr>\n",
							"    <tr>\n",
							"      <th>4</th>\n",
							"      <td>blocks.0.self_attention.heads.0.value</td>\n",
							"      <td>Linear</td>\n",
							"      <td>24576</td>\n",
							"      <td>24576</td>\n",
							"    </tr>\n",
							"    <tr>\n",
							"      <th>...</th>\n",
							"      <td>...</td>\n",
							"      <td>...</td>\n",
							"      <td>...</td>\n",
							"      <td>...</td>\n",
							"    </tr>\n",
							"    <tr>\n",
							"      <th>191</th>\n",
							"      <td>blocks.5.feed_forward.net.3</td>\n",
							"      <td>Dropout</td>\n",
							"      <td>0</td>\n",
							"      <td>0</td>\n",
							"    </tr>\n",
							"    <tr>\n",
							"      <th>192</th>\n",
							"      <td>blocks.5.layer_norm_1</td>\n",
							"      <td>LayerNorm</td>\n",
							"      <td>768</td>\n",
							"      <td>768</td>\n",
							"    </tr>\n",
							"    <tr>\n",
							"      <th>193</th>\n",
							"      <td>blocks.5.layer_norm_2</td>\n",
							"      <td>LayerNorm</td>\n",
							"      <td>768</td>\n",
							"      <td>768</td>\n",
							"    </tr>\n",
							"    <tr>\n",
							"      <th>194</th>\n",
							"      <td>final_layer_norm</td>\n",
							"      <td>LayerNorm</td>\n",
							"      <td>768</td>\n",
							"      <td>768</td>\n",
							"    </tr>\n",
							"    <tr>\n",
							"      <th>195</th>\n",
							"      <td>final_linear_layer</td>\n",
							"      <td>Linear</td>\n",
							"      <td>398090</td>\n",
							"      <td>398090</td>\n",
							"    </tr>\n",
							"  </tbody>\n",
							"</table>\n",
							"<p>196 rows × 4 columns</p>\n",
							"</div>"
						],
						"text/plain": [
							"                                Layer Name       Type  Parameters  Trainable\n",
							"0                    token_embedding_table  Embedding      397056     397056\n",
							"1                 position_embedding_table  Embedding       98304      98304\n",
							"2      blocks.0.self_attention.heads.0.key     Linear       24576      24576\n",
							"3    blocks.0.self_attention.heads.0.query     Linear       24576      24576\n",
							"4    blocks.0.self_attention.heads.0.value     Linear       24576      24576\n",
							"..                                     ...        ...         ...        ...\n",
							"191            blocks.5.feed_forward.net.3    Dropout           0          0\n",
							"192                  blocks.5.layer_norm_1  LayerNorm         768        768\n",
							"193                  blocks.5.layer_norm_2  LayerNorm         768        768\n",
							"194                       final_layer_norm  LayerNorm         768        768\n",
							"195                     final_linear_layer     Linear      398090     398090\n",
							"\n",
							"[196 rows x 4 columns]"
						]
					},
					"execution_count": 13,
					"metadata": {},
					"output_type": "execute_result"
				}
			],
			"source": [
				"import pandas as pd\n",
				"\n",
				"\n",
				"def get_model_stats(model: torch.nn.Module) -> pd.DataFrame:\n",
				"    \"\"\"\n",
				"    Create a DataFrame with detailed layer statistics\n",
				"    \"\"\"\n",
				"    stats = []\n",
				"    for name, module in model.named_modules():\n",
				"        if len(list(module.children())) == 0:  # Only leaf modules\n",
				"            params = sum(p.numel() for p in module.parameters())\n",
				"            stats.append({\n",
				"                'Layer Name': name,\n",
				"                'Type': module.__class__.__name__,\n",
				"                'Parameters': params,\n",
				"                'Trainable': sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
				"            })\n",
				"    return pd.DataFrame(stats)\n",
				"\n",
				"\n",
				"stats_df = get_model_stats(model)\n",
				"stats_df"
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Python 3",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "ipython",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "python",
			"nbconvert_exporter": "python",
			"pygments_lexer": "ipython3",
			"version": "3.12.2"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 2
}
