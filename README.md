ðŸ§  Personalized LLM Trained on WhatsApp Chats

Fine-tuned transformer-based language model using personal chat data to emulate individual conversational style.

ðŸ“Œ Project Overview

This project involves building a custom Large Language Model (LLM) that mimics my personal communication style by training it on 51k tokens extracted from my WhatsApp chats. It leverages transformer-based architectures, byte pair encoding (BPE) tokenization, LangChain for dynamic context management, and instruction + LoRA fine-tuning techniques.

â¸»

ðŸ”§ Tech Stack
	â€¢	Python
	â€¢	LangChain
	â€¢	Transformers (HuggingFace)
	â€¢	LoRA (Low-Rank Adaptation)
	â€¢	BPE Tokenization
	â€¢	Jupyter Notebooks

â¸»

ðŸ“Š Key Features
	â€¢	Tokenizes personal WhatsApp chat history using the Byte Pair Encoding (BPE) algorithm.
	â€¢	Pre-trains a base transformer model on sequential dialogue data.
	â€¢	Creates a curated instruction-tuning dataset for supervised fine-tuning.
	â€¢	Applies LoRA to reduce computational cost while achieving high personalization.
	â€¢	Integrates with LangChain to maintain conversation memory and contextual awareness.

â¸»

ðŸ§ª Workflow
	1.	Data Extraction: Parsed and cleaned WhatsApp chat data to form training corpus.
	2.	Tokenization: Applied BPE to encode text into efficient token format.
	3.	Pre-Training: Initialized transformer model on generic text patterns.
	4.	Fine-Tuning:
	â€¢	Instruction tuning using custom prompts
	â€¢	LoRA applied for lightweight fine-tuning
	5.	Deployment (WIP): Planning LangChain agent integration for interactive chat via CLI or web.

ðŸš€ Future Improvements
	â€¢	Add a web interface using Flask or FastAPI.
	â€¢	Evaluate response accuracy using BLEU/ROUGE metrics.
	â€¢	Integrate real-time inference API for demo purposes.

â¸»

ðŸ§  Inspiration

Inspired by the desire to personalize AI assistants and explore lightweight, domain-specific LLM training using real-world private datasets.

â¸»

ðŸ“œ License

This project is for educational and personal research use only. All training data is anonymized and not distributed.
